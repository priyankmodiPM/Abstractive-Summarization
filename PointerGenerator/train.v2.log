[2019-11-25 21:46:27,338 INFO] Loading train dataset from data/gigaword/PREPROCESSED.train.0.pt, number of examples: 200000
[2019-11-25 21:46:27,423 INFO]  * vocabulary size. source = 50004; target = 50004
[2019-11-25 21:46:27,423 INFO] Building model...
[2019-11-25 21:46:41,913 INFO] NMTModel(
  (encoder): RNNEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(50004, 128, padding_idx=1)
        )
      )
    )
    (rnn): LSTM(128, 256, num_layers=2, dropout=0.1, bidirectional=True)
    (bridge): ModuleList(
      (0): Linear(in_features=512, out_features=512, bias=True)
      (1): Linear(in_features=512, out_features=512, bias=True)
    )
  )
  (decoder): InputFeedRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(50004, 128, padding_idx=1)
        )
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.1, inplace=False)
      (layers): ModuleList(
        (0): LSTMCell(640, 512)
        (1): LSTMCell(512, 512)
      )
    )
    (attn): GlobalAttention(
      (linear_context): Linear(in_features=512, out_features=512, bias=False)
      (linear_query): Linear(in_features=512, out_features=512, bias=True)
      (v): Linear(in_features=512, out_features=1, bias=False)
      (linear_out): Linear(in_features=1024, out_features=512, bias=True)
    )
  )
  (generator): CopyGenerator(
    (linear): Linear(in_features=512, out_features=50004, bias=True)
    (linear_copy): Linear(in_features=512, out_features=1, bias=True)
    (softmax): Softmax(dim=1)
    (sigmoid): Sigmoid()
  )
)
[2019-11-25 21:46:41,928 INFO] encoder: 9293312
[2019-11-25 21:46:41,928 INFO] decoder: 37567829
[2019-11-25 21:46:41,928 INFO] * number of parameters: 46861141
[2019-11-25 21:46:41,954 INFO] Start training...
